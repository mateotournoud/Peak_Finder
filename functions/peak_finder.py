#-----------------------LIBRARIES--------------------------------
import pandas as pd
import numpy as np
from scipy.signal import find_peaks
from scipy.interpolate import UnivariateSpline

#----------------------------------------------------------------
#------------------------IMPORT-DATASET--------------------------
def import_simulation(path, norm_between=None):
    """
    This function imports a simulated dataset generated by the NIST Libs database
    (https://physics.nist.gov/PhysRefData/ASD/LIBS/libs-form.html). All intensity
    columns (e.g., per species/ionization state) are summed into a single 'Spectrum'
    column.
    
    The spectrum is shifted in the intensity scale, matching the minimum intensity
    to 0. After that, the normalization of the data can be done using the highest
    peak of the dataset. When working with several simulations, it is also possible
    to normalize them using the height of a specific peak, given the peak boundaries
    in nanometers. The chosen peak should correspond to an element whose percentage
    in the mixture is constant along all the simulations of the dataset.
    
    Parameters
    ----------
    path : str
        It is the Windows file path of the file that is to be imported. It has to
        be written as a raw string literal (for example, r'C:\path\to\file.txt')
    norm_between : tuple, optional
        When the normalization is made with respect to a peak, this parameter must
        include the wavelengths at the left and at the right of the peak, given in
        nanometers. The normalization height will be the highest data point between
        the given boundaries. The default is None.

    Returns
    -------
    data : pandas.core.frame.DataFrame
        The simulation is imported as a dataframe of two columns, where the first
        one is called "Wavelength" and represents the location of the emission
        lines, and the second one is named "Spectrum" and contains the intensity
        of each of those spectral lines.

    """
    data = pd.read_csv(path, sep=',',header=0)
    data.iloc[:, 1] = data.iloc[:, 2:].sum(axis=1)
    data = data.iloc[:, :2]
    data.columns = ["Wavelength", "Spectrum"]
    under_zero = min(data["Spectrum"])
    data["Spectrum"] = data["Spectrum"]-under_zero
    if norm_between==None:
        data["Spectrum"] = data["Spectrum"]/max(data["Spectrum"])
    else:
        mask = (data["Wavelength"] > norm_between[0]) & (data["Wavelength"] < norm_between[1])
        norm_peak = data.loc[mask, "Spectrum"]
        norm_height = norm_peak.max()
        data["Spectrum"] = data["Spectrum"]/norm_height
    return data



def import_measurement(path):
    return pd.read_csv(path, sep=',',header=0)


#----------------------------------------------------------------
#--------------DETECT-THE-PEAKS-AND-THE-HALF-PEAKS---------------
def findpeaks(data, category_clearly=0.2, category_visible=0.06, category_small=0.001, prom_weight=4):
    """
    The findpeaks function uses the scipy.signal.find_peaks() package to detect
    peaks in the data. The peak finding boundaries are fixed to prominence=0,
    width=0, rel_height=0.9 and plateau_size=1, so that SciPy computes the
    largest possible number of peaks, and returns their properties in 'props'.
    For more information about the algorithm logic of the scipy.signal.find_peaks
    function, please refer to its documentation.
    
    The peaks properties stored in 'props' are used to label the peaks into three
    different categories: "clearly visible", "visible" or "small". Every peak
    that does not fulfill the requirements for any of these categories is labeled
    as "noise".
    
    The categorization is made with the "category_factor", which is the weighted
    harmonic mean between the peak height and the peak prominence. The weight of
    the peak height is fixed to 1, while the prominence weight is controlled by
    the prom_weight input parameter. For some measurements, it might make sence
    to use a prom_weight of 7 or 10.

    Parameters
    ----------
    data : pandas.core.frame.DataFrame
        It is the dataset where to look for peaks, imported as a dataframe. The
        function will only use two of its columns, which must be named as:
        "Wavelength" and "Spectrum". 
    category_clearly : float, optional
        All the peaks whose category factor is greater than or equal to this value, 
        will be labeled "clearly visible". Default is 0.2.
    category_visible : float, optional
        All the peaks whose category factor is greater than or equal to this value,
        will be labeled "visible". Default is 0.06.
    category_small : float, optional
        All the peaks whose category factor is greater than or equal to this value,
        will be labeled "small". All the peaks whose category factor is smaller than
        this value, will be labeled "noise". Default is 0.001.
    prom_weight : int, optional
        This factor modifies the weight of the prominence when calculating the
        category factor β. Default is 4.
        
    -----------------------------------------------
    |     (prom_weight + 1) * prominence * height |
    | β = --------------------------------------- |
    |      prom_weight * prominence + 1 * height  |
    -----------------------------------------------

    Raises
    ------
    ValueError
        Input parameter prom_weight cannot be equal to 0.

    Returns
    -------
    data : pandas.core.frame.DataFrame
        The dataframe given as an input, is added three new columns:
        - Peaks: is the height of the detected peak, taken directly from the
        "Spectrum" column. 
        - Prominence: the peak prominence, calculated by the find_peaks
        scipy.signal function, is stored in this column.
        - Category: these are the peak labels: "clearly_visible", "visible",
        "small" and/or "noise".
    """
    if prom_weight==0:
        raise ValueError("Input parameter prom_weight cannot be equal to 0.")
    peaks, props = find_peaks(data["Spectrum"], prominence=0, width=0, plateau_size=1, rel_height=0.9)
    data.loc[peaks, "Peaks"] = data.loc[peaks, "Spectrum"]
    data.loc[peaks, "Prominence"] = props['prominences']
    data.loc[peaks, "Category"] = 'noise'
    for idx in data.index[~data['Prominence'].isna()]:
        prom_value = data.loc[idx, "Prominence"]
        peak_height = data.loc[idx, "Peaks"]
        category_factor = (prom_weight+1) * prom_value * peak_height / (prom_weight*prom_value + peak_height)
        if category_small <= category_factor:
            data.loc[idx, "Category"] = 'small'
            if category_visible <= category_factor:
                data.loc[idx, "Category"] = 'visible'
                if category_clearly <= category_factor:
                    data.loc[idx, "Category"] = 'clearly_visible'
    return data


def halfpeak_finder(data, big_prom=0.1, big_width=1.0, small_prom=0.003, small_width=0.3,
                    ss=10, k=3, height_thr=0.1, denoiser=False):
    """
    Detects 'half peaks' on the first derivative using two passes:
        1) DOWNSLOPE pass: find local *maxima below zero* directly on dI/dλ; map
        to RIGHT row (k+1).
        2) UPSLOPE pass: find local *maxima below zero* on the *inverted*
        derivative; map to LEFT row (k).

    The first derivative of the spectrum is built using finite differences at a
    spline of the spectrum, where S=len(wavelength)*10**(-ss) is the smoothing
    factor used to choose the number of knots at the spline, and K is the degree
    of the smoothing spline.

    Peak finding boundaries are fixed to height=(-height_thr, 0), prominence=0,
    width=0 and rel_height=0.9 so that SciPy computes the largest possible
    number of half peaks, and returns their properties in 'props'. These peak
    properties are used to label the peaks into two different categories: "big"
    or "small". Every half peak that does not match any of these categories is
    labeled as "noise".

    The categorization is made by comparing the peaks prominence and width with
    the given inputs:
    - "big" if (big_prom <= prominence) and (big_width <= width)
    - "small" if (small_prom <= prominence < big_prom) and
      (small_width <= width < big_width)
    - "noise" for any other case

    Parameters
    ----------
    data : pandas.core.frame.DataFrame
        It is the dataset where to look for half peaks, imported as a dataframe.
        The function will only use two of its columns, which must be named as:
        "Wavelength" and "Spectrum".
    big_prom : float, big_width : float
        The function will label as "big" to every half peak whose prominence is
        larger or equal than "big_prom" and whose width is larger or equal than
        "big_width" (given in nanometers). The default values are 0.1 and 1.0,
        respectively. 
    small_prom : float, small_width : float
        The function will label as "small" to every half peak which is not already
        labeled as "big", whose prominence is larger or equal than "small_prom"
        and whose width is larger or equal than "small_width" (given in nanometers).
        The default values are 0.003 and 0.3, respectively. 
    ss : int, optional
        It is the exponential used to calculate the positive smoothing factor of
        the spline. Decreasing this value will provide a smoother spectrum. It is
        recommended to set it between 6 (maximum of smooth) and 10 (maximum of
        raw). The default is 10.
    k : int, optional
        It is the degree of smoothing spline. It must be between [1,5], where k=3
        is a cubic spline. The default is 3. Recommendation:
        - Set k=2 for measurements made with 'average exposures' equal to 32.
        - Set k=3 for NIST simulations.
        - Set k=4 for measurements made without 'average exposures'.
    height_thr : float, optional
        This input parameter excludes false hits that are detected at regions of
        the spectra where the first derivative is not close to 0. The default is
        0.1. It is not recommended to reduce this value without adjusting the shape
        of the first derivative first, varying 's' and 'k' accordingly.
    denoiser : bool, optional
        If 'True', all the half peaks labeled as noise are deleted from the output
        dataframe. The default is False.

    Raises
    ------
    ValueError
        height_thr must be a positive number larger than 0.
        ss must be a positive integer larger than 0.

    Returns
    -------
    data : pandas.core.frame.DataFrame
        The dataframe given as an input, is added two new columns:
        - Halfpeaks: contains the height of the detected half peak, taken directly
        from the "Spectrum" column. 
        - Category: these are the half peak labels: "big", "small", and/or "noise".
    derivative1st : np.ndarray shape (N-1, 2)
        Columns: [midpoint_wavelength, normalized_derivative].
        The first derivative can be plotted with the function "plot_derivative"
        from the peak_finder_plot_library.
    """
    if height_thr <=0:
        raise ValueError("height_thr must be a positive number larger than 0")
    if ss<=0:
        raise ValueError("ss must be a positive integer larger than 0")
    if ss<6 or ss>10:
        print("Alert: 'ss' is recommended to be between 6 and 10.")
    wavelength = data["Wavelength"].to_numpy()
    spectrum = data["Spectrum"].to_numpy()

    midpoints1 = (wavelength[:-1] + wavelength[1:]) / 2.0
    spl = UnivariateSpline(wavelength, spectrum, s=len(wavelength)*10**-ss, k=k)
    spectrum_diff1 = (spl(midpoints1 + 1e-9) - spl(midpoints1 - 1e-9)) / (2e-9)
    spectrum_diff1 /= np.max(np.abs(spectrum_diff1))
    derivative1st = np.column_stack((midpoints1, spectrum_diff1))

    # --- Detect halfpeaks at the downslope ---
    pk_downslope, props_downslope = find_peaks(spectrum_diff1, height=(-height_thr,0), prominence=0, width=0, rel_height=0.9)
    right_indices = (pk_downslope + 1).clip(min=0, max=len(spectrum)-1)
    
    # --- Detect halfpeaks at the upslope ---
    spectrum_diff1 = -spectrum_diff1
    pk_upslope, props_upslope = find_peaks(spectrum_diff1, height=(-height_thr,0), prominence=0, width=0, rel_height=0.9)
    left_indices = (pk_upslope).clip(min=0, max=len(spectrum)-1)

    # --- Extract the peaks width from "left_ips" and "right_ips" ---
    left_ips_downslope = props_downslope.get("left_ips",  np.array([]))
    right_ips_downslope = props_downslope.get("right_ips", np.array([]))
    left_ips_upslope = props_upslope.get("left_ips",  np.array([]))
    right_ips_upslope = props_upslope.get("right_ips", np.array([]))
    
    # --- Clip each float ips to the closest existing integer index ---
    max_idx = len(midpoints1) - 1
    idx_left_downslope = np.clip(np.rint(left_ips_downslope).astype(int), 0, max_idx)
    idx_right_downslope = np.clip(np.rint(right_ips_downslope).astype(int), 0, max_idx)
    idx_left_upslope = np.clip(np.rint(left_ips_upslope).astype(int), 0, max_idx)
    idx_right_upslope = np.clip(np.rint(right_ips_upslope).astype(int), 0, max_idx)

    # --- Obtain the wavelength of each ips ---
    wl_left_downslope = midpoints1[idx_left_downslope]
    wl_right_downslope = midpoints1[idx_right_downslope]
    wl_left_upslope = midpoints1[idx_left_upslope]
    wl_right_upslope = midpoints1[idx_right_upslope]

    # --- Calculate the absolute wavelength width between right and left at rel_height ---
    w_down = np.abs(wl_right_downslope - wl_left_downslope)
    w_up = np.abs(wl_right_upslope - wl_left_upslope)
    p_down = props_downslope.get("prominences", np.array([]))
    p_up = props_upslope.get("prominences", np.array([]))
    
    # --- BIG HALF PEAKS: both conditions individually pass ---
    big_down = (p_down >= big_prom) & (w_down >= big_width) if p_down.size else np.array([], dtype=bool)
    big_up = (p_up >= big_prom) & (w_up >= big_width) if p_up.size else np.array([], dtype=bool)
    small_down = (p_down >= small_prom) & (w_down >= small_width) & (~big_down) if p_down.size else np.array([], dtype=bool)
    small_up = (p_up >= small_prom) & (w_up >= small_width) & (~big_up) if p_up.size else np.array([], dtype=bool)
    
    # --- Labeling: initialize with 'noise', then upgrade to 'small' or 'big' ---
    labels_down = np.full(w_down.shape, "noise", dtype=object) if w_down.size else np.array([], dtype=object)
    labels_up   = np.full(w_up.shape,   "noise", dtype=object) if w_up.size   else np.array([], dtype=object)
    labels_down[small_down] = "small"
    labels_down[big_down]   = "big"
    labels_up[small_up]     = "small"
    labels_up[big_up]       = "big"

    # --- Optionally denoise by removing "noise" ---
    if denoiser:
        keep_down = (labels_down != "noise")
        keep_up   = (labels_up   != "noise")
    else:
        keep_down = np.ones_like(labels_down, dtype=bool)
        keep_up   = np.ones_like(labels_up,   dtype=bool)
    right_indices = right_indices[keep_down]
    labels_down   = labels_down[keep_down]
    left_indices  = left_indices[keep_up]
    labels_up     = labels_up[keep_up]

    # ---- Combine labels onto a per-row Series; "big" wins over "small" on collision ---
    category_out = pd.Series(index=data.index, dtype=object)

    for idx, lab in zip(right_indices, labels_down): # Fill downslope first
        prev = category_out.iat[idx]
        if pd.isna(prev) or lab == "big" or prev == "small":
            category_out.iat[idx] = lab

    for idx, lab in zip(left_indices, labels_up): # Then upslope
        prev = category_out.iat[idx]
        if pd.isna(prev) or lab == "big" or prev == "small":
            category_out.iat[idx] = lab

    # ---- Build boolean masks to return (already reflect denoiser) ----
    mask_right = np.zeros(len(spectrum), dtype=bool)
    mask_left  = np.zeros(len(spectrum), dtype=bool)
    if right_indices.size:
        mask_right[right_indices] = True
    if left_indices.size:
        mask_left[left_indices] = True
    
    # --- Export dataframe ---
    data.loc[mask_right, "Halfpeaks"] = data.loc[mask_right, "Spectrum"].to_numpy()
    data.loc[mask_left, "Halfpeaks"] = data.loc[mask_left, "Spectrum"].to_numpy()
    data.loc[category_out.notna(), "Category"] = category_out.loc[category_out.notna()].to_numpy()
    return data, derivative1st